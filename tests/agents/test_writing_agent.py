import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch
from datetime import datetime, timezone
import logging

from agents.writing_agent import WritingAgent
from agents.base_agent import Task, Artifact, TaskStatus # For creating test tasks/artifacts
from protocols.a2a_schemas import AgentMessage
from core.agent_prompt_builder import generate_prompt # To verify prompt construction if needed

@pytest.fixture
async def writing_agent_instance():
    """Provides a WritingAgent instance with a mocked OpenAI client."""
    with patch('agents.writing_agent.AsyncOpenAI') as MockOpenAIClass:
        mock_openai_client_instance = AsyncMock()
        MockOpenAIClass.return_value = mock_openai_client_instance
        instance = WritingAgent()
        # instance.openai_client is now correctly set by the __init__ method
        # due to the patch, to mock_openai_client_instance.
        yield instance

@pytest.mark.asyncio
async def test_writing_agent_initialization(writing_agent_instance: WritingAgent):
    assert writing_agent_instance.card.name == "Writing Agent"
    assert "write_content" in [cap.skill_name for cap in writing_agent_instance.card.capabilities]
    assert writing_agent_instance.openai_client is not None
    assert isinstance(writing_agent_instance.openai_client, AsyncMock)
    # Ensure the specific instance from the patch is used
    assert writing_agent_instance.openai_client == WritingAgent._patch_default_original_openai_client if hasattr(WritingAgent, '_patch_default_original_openai_client') else writing_agent_instance.openai_client

@pytest.mark.asyncio
async def test_process_task_success(writing_agent_instance: WritingAgent):
    # Mock the OpenAI client's chat completion response
    mock_completion_response = MagicMock()
    mock_completion_response.choices = [MagicMock()]
    mock_completion_response.choices[0].message.content = "# Test Blog Post\nThis is a test draft."
    mock_completion_response.usage = MagicMock() # Ensure usage is a MagicMock
    mock_completion_response.usage.prompt_tokens = 100
    mock_completion_response.usage.completion_tokens = 50
    mock_completion_response.usage.total_tokens = 150
    # The openai_client on the instance IS the AsyncMock, so we configure its methods
    writing_agent_instance.openai_client.chat.completions.create = AsyncMock(return_value=mock_completion_response)

    research_data = "Some research findings about topic X."
    research_artifact = Artifact(
        artifact_id="research_artifact_1",
        task_id="task_for_research",
        creator_agent_id="research_agent",
        content_type="text/markdown",
        data=research_data,
        description="Web research summary for topic: Topic X",
        created_at=datetime.now(timezone.utc).isoformat()
    )
    task = writing_agent_instance.create_task(
        description="Write a blog post on Topic X",
        initiator_agent_id="orchestrator",
        input_artifacts=[research_artifact]
    )
    writing_agent_instance.set_message_handler(AsyncMock())
    await writing_agent_instance.process_task(task)

    assert task.status == TaskStatus.COMPLETED
    assert len(task.output_artifacts) == 1
    output_artifact = task.output_artifacts[0]
    assert output_artifact.content_type == "text/markdown"
    assert "# Test Blog Post" in output_artifact.data
    assert "(generated by OpenAI, prompt builder)" in output_artifact.description

    writing_agent_instance.openai_client.chat.completions.create.assert_awaited_once()
    call_args = writing_agent_instance.openai_client.chat.completions.create.call_args
    assert call_args.kwargs['model'] == "gpt-3.5-turbo"
    messages = call_args.kwargs['messages']
    assert messages[0]["role"] == "system"
    assert messages[1]["role"] == "user"
    assert "topic x" in messages[1]["content"].lower()
    assert research_data in messages[1]["content"]

    writing_agent_instance.message_handler.assert_awaited_once()
    sent_message = writing_agent_instance.message_handler.call_args[0][0]
    assert sent_message.message_type == "task_status_update"
    assert sent_message.payload["task_id"] == task.task_id

@pytest.mark.asyncio
async def test_process_task_no_input_artifacts(writing_agent_instance: WritingAgent, caplog):
    caplog.set_level(logging.ERROR)
    task = writing_agent_instance.create_task(
        initiator_agent_id="orchestrator",
        description="Write blog post"
    )
    writing_agent_instance.set_message_handler(AsyncMock())
    await writing_agent_instance.process_task(task)

    assert task.status == TaskStatus.FAILED
    assert f"Writing task {task.task_id} for Writing Agent has no input research artifact" in caplog.text
    writing_agent_instance.message_handler.assert_awaited_once()

@pytest.mark.asyncio
async def test_process_task_openai_api_error(writing_agent_instance: WritingAgent, caplog):
    caplog.set_level(logging.ERROR)
    writing_agent_instance.openai_client.chat.completions.create = AsyncMock(side_effect=Exception("OpenAI API Error"))

    research_artifact = Artifact(
        artifact_id="res1", task_id="t1", creator_agent_id="r",
        content_type="text/plain", data="Research data",
        description="topic: Test Topic API Error",
        created_at=datetime.now(timezone.utc).isoformat()
    )
    task = writing_agent_instance.create_task(
        description="Blog about API error",
        initiator_agent_id="orch",
        input_artifacts=[research_artifact]
    )
    writing_agent_instance.set_message_handler(AsyncMock())
    await writing_agent_instance.process_task(task)

    assert task.status == TaskStatus.COMPLETED
    assert "Error calling OpenAI API for topic 'Test Topic API Error'" in caplog.text
    assert "OpenAI API Error" in caplog.text
    assert len(task.output_artifacts) == 1
    assert "Error-Fallback Blog Post" in task.output_artifacts[0].data
    writing_agent_instance.message_handler.assert_awaited_once()

@pytest.mark.asyncio
async def test_process_task_openai_client_not_initialized(caplog):
    with patch('agents.writing_agent.AsyncOpenAI', side_effect=Exception("Init failed")):
        with caplog.at_level(logging.ERROR):
            agent = WritingAgent()
    
    assert agent.openai_client is None
    assert "Error initializing OpenAI client" in caplog.text
    assert "Init failed" in caplog.text

    caplog.clear()
    caplog.set_level(logging.WARNING)

    research_artifact = Artifact(
        artifact_id="res2", task_id="t2", creator_agent_id="r",
        content_type="text/plain", data="Research data",
        description="topic: Client None Test",
        created_at=datetime.now(timezone.utc).isoformat()
    )
    task = agent.create_task(
        description="Blog with no client",
        initiator_agent_id="orch",
        input_artifacts=[research_artifact]
    )
    agent.set_message_handler(AsyncMock())
    await agent.process_task(task)

    assert task.status == TaskStatus.COMPLETED 
    assert len(task.output_artifacts) == 1
    assert "Simulated Blog Post" in task.output_artifacts[0].data
    assert "OpenAI client not available. Using SIMULATED draft generation" in caplog.text
    agent.message_handler.assert_awaited_once()

@pytest.mark.asyncio
async def test_writing_agent_topic_extraction_from_description(writing_agent_instance: WritingAgent):
    writing_agent_instance.generate_blog_draft_with_openai = AsyncMock(return_value="Mocked Draft")
    
    test_cases = [
        ("Web research summary for topic: My Awesome Topic", "My Awesome Topic"),
        ("Research summary for topic: Another Topic (detailed)", "Another Topic"),
        ("Some other description without the keyword", "the provided research"),
        ("topic: Only Topic Here", "Only Topic Here"),
        ("No topic keyword in this string", "the provided research")
    ]

    for desc, expected_topic in test_cases:
        research_artifact = Artifact(
            artifact_id="res_topic_test", task_id="t_topic", creator_agent_id="r",
            content_type="text/plain", data="Data", description=desc,
            created_at=datetime.now(timezone.utc).isoformat()
        )
        task = writing_agent_instance.create_task(
            description="Blog on whatever",
            initiator_agent_id="orch",
            input_artifacts=[research_artifact]
        )
        # Reset message_handler mock for each iteration if it's checked per call
        # For generate_blog_draft_with_openai, we are checking its calls at the end.
        writing_agent_instance.message_handler = AsyncMock() # Reset for this specific call to process_task
        
        await writing_agent_instance.process_task(task)
        
        writing_agent_instance.generate_blog_draft_with_openai.assert_any_call(
            "Data",
            expected_topic
        )
        writing_agent_instance.message_handler.assert_awaited_once() # Check message sent for this task

    assert writing_agent_instance.generate_blog_draft_with_openai.call_count == len(test_cases) 